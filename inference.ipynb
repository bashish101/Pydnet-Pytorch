{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of Pydnet-Pytorch pretrained model on an image\n",
    "\n",
    "Run inference on an image and display the output and test the runtime on local machine: Pytorch 1.6, CUDA 10.2 SuperServer with GTX 2080 TI GPU.\n",
    "\n",
    "Also optimize the model for faster inference using \n",
    "\n",
    "- [x] JIT from Pytorch\n",
    "- [ ] Jit from NVIDIA (TrTorch)\n",
    "- [ ] Mobile optimizer of Pytorch  --> Gives error. Try blacklisting each of the optimization and check if error disappears\n",
    "- [ ] ONNX and it's optimizer\n",
    "- [ ] ONNX runtime\n",
    "- [ ] TensorRT\n",
    "- [ ] TVM\n",
    "- [ ] Convert ONNX model to Tenseoflow and compare runtime. must have align_corners=True in upsample for this (requires retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from pydnet import PyddepthInference, Pydnet\n",
    "import torch.utils.mobile_optimizer as mobile_optimizer\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.open('test/1.png')\n",
    "img1 = img1.resize((640, 192), Image.ANTIALIAS)\n",
    "example1 = TF.to_tensor(img1).unsqueeze_(0).to(device)\n",
    "\n",
    "print(example1.shape)\n",
    "plt.imshow(img1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bla=Pydnet(mobile_version=True, my_version=False)\n",
    "# loaded_dict = torch.utils.model_zoo.load_url(\"https://github.com/zshn25/Pydnet-Pytorch/blob/forMonodepth2/mobile_pydnet.pth\", \n",
    "#                                              map_location= lambda storage, loc: storage)\n",
    "\n",
    "#bla.load_state_dict(loaded_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyddepth = PyddepthInference(mobile_version=True, my_version=False, pretrained=False)\n",
    "\n",
    "loaded_dict_enc = torch.load(\"mobile_pydnet.pth\", map_location=device)\n",
    "\n",
    "# pyddepth = PyddepthInference(mobile_version=True, my_version=True, pretrained=False)\n",
    "\n",
    "# loaded_dict_enc = torch.load(\"my_mobile_pydnet.pth\", map_location=device)\n",
    "\n",
    "new_dict_enc = {}\n",
    "for k,v in loaded_dict_enc.items():\n",
    "    new_dict_enc[k.replace(\"module.\", \"\")] = loaded_dict_enc[k]\n",
    "    \n",
    "pyddepth.load_state_dict(new_dict_enc, strict=False)\n",
    "pyddepth.to(device)\n",
    "pyddepth.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit with torch.no_grad(): pyddepth(example1)\n",
    "\n",
    "output=pyddepth(example1)\n",
    "output1 = output.to('cpu').detach().numpy()\n",
    "output1 = output1.squeeze()\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Collect parameters to prune\n",
    "parameters_to_prune = ()\n",
    "for name, module in pyddepth.named_modules():\n",
    "      if hasattr(module, \"weight\"):\n",
    "        parameters_to_prune += ((module, 'weight'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.35\n",
    ")\n",
    "\n",
    "for p,_ in parameters_to_prune:\n",
    "    prune.remove(p, name=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit with torch.no_grad(): pyddepth(example1)\n",
    "\n",
    "output=pyddepth(example1)\n",
    "output1 = output.to('cpu').detach().numpy()\n",
    "output1 = output1.squeeze()\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pyddepth, \"mobile_pydnet_pruned35.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Optimization for faster inference\n",
    "\n",
    "### Jit Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(pyddepth, example1)\n",
    "%timeit traced_model(example1)\n",
    "\n",
    "output=traced_model(example1)\n",
    "output1 = output.to('cpu').detach().numpy()\n",
    "output1 = output1.squeeze()\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripted_model = torch.jit.script(pyddepth, example1)\n",
    "#%timeit scripted_model(example1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIT Traced model is faster than original model. Using JIT traced from now on.\n",
    "\n",
    "### Pytorch's mobile optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_traced_model = mobile_optimizer.optimize_for_mobile(traced_model)\n",
    "\n",
    "#%timeit with torch.no_grad(): optimized_traced_model(example1) # check https://discuss.pytorch.org/t/runtimeerror-mobile-optimized-model-cannot-be-inferenced-on-gpu/94098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model. \n",
    "# Using opset version 11 as the model contains nn.Upsample (which is supported by opset version >=11)\n",
    "onnx_model = torch.onnx.export(traced_model,               # model being run\n",
    "                              example1,                         # model input (or a tuple for multiple inputs)\n",
    "                              \"mobile_pydnet.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                              example_outputs=output,\n",
    "                              export_params=True,        # store the trained parameter weights inside the model file\n",
    "                              opset_version=11,          # the ONNX version to export the model to\n",
    "                              keep_initializers_as_inputs=True,\n",
    "                              do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                              input_names = ['input'],   # the model's input names\n",
    "                              output_names = ['output'], # the model's output names\n",
    "                              dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                                            'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing onnxruntime without GPU as it requires CUDA 10.1 but 10.2 is installed\n",
    "!pip install onnxruntime onnx --upgrade\n",
    "#!conda install -c conda-forge onnx --yes\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"mobile_pydnet.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "#onnx.helper.printable_graph(model.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "#netron.start(\"mobile_pydnet.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX Optimizer: See [this](https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#optimizing-an-onnx-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import optimizer\n",
    "\n",
    "onnx.checker.check_model(model)\n",
    "onnx.helper.strip_doc_string(model)\n",
    "optimized_model = onnx.shape_inference.infer_shapes(model)\n",
    "\n",
    "optimizers_list = ['eliminate_deadend', 'eliminate_nop_dropout',\n",
    "                                            'eliminate_nop_monotone_argmax', 'eliminate_nop_pad',\n",
    "                                            'extract_constant_to_initializer', 'eliminate_unused_initializer',\n",
    "                                            'eliminate_nop_transpose', 'fuse_add_bias_into_conv',\n",
    "                                            'fuse_consecutive_concats', 'eliminate_identity',\n",
    "                                            'fuse_consecutive_log_softmax',\n",
    "                                            'fuse_consecutive_reduce_unsqueeze', 'fuse_consecutive_squeezes',\n",
    "                                            'fuse_consecutive_transposes', 'fuse_matmul_add_bias_into_gemm',\n",
    "                                            'fuse_pad_into_conv', 'fuse_transpose_into_gemm', 'fuse_bn_into_conv']\n",
    "optimized_model = optimizer.optimize(optimized_model, optimizers_list,#optimizer.get_available_passes(),\n",
    "                                     fixed_point=True)\n",
    "onnx.checker.check_model(optimized_model)\n",
    "\n",
    "onnx.save(optimized_model, \"optimized_mobile_pydnet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ort_input = example1.to('cpu').detach().numpy().astype(np.float32)\n",
    "\n",
    "ort_session = ort.InferenceSession('optimized_mobile_pydnet.onnx')\n",
    "\n",
    "print(\"This is on CPU as ONNXRuntime is on CPU\")\n",
    "%timeit outputs = ort_session.run(None,  {ort_session.get_inputs()[0].name: ort_input})[0]\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow \n",
    "#!pip install tensorflow-addons\n",
    "# Install onnx-tensorflow as follows in terminal\n",
    "#!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow\n",
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "model = onnx.load(\"mobile_pydnet_interp.onnx\")\n",
    "\n",
    "tf_rep = prepare(model)\n",
    "\n",
    "# Input nodes to the model\n",
    "print('inputs:', tf_rep.inputs)\n",
    "\n",
    "# Output nodes from the model\n",
    "print('outputs:', tf_rep.outputs)\n",
    "\n",
    "# All nodes in the model\n",
    "print('tensor_dict:')\n",
    "print(tf_rep.tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is on CPU as ONNXRuntime is on CPU\")\n",
    "%timeit output1 = tf_rep.run(example1.to('cpu').detach().numpy().astype(np.float32))\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rep.export_graph('mobile_pydnet.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
