{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of Pydnet-Pytorch pretrained model on an image\n",
    "\n",
    "Run inference on an image and display the output and test the runtime on local machine: Pytorch 1.7, CUDA 10.1 computer with GTX 2080 TI GPU, Tensorflow 2.4.0-dev.\n",
    "\n",
    "Also optimize the model for faster inference using \n",
    "\n",
    "- [x] JIT from Pytorch\n",
    "- [ ] Jit from NVIDIA (TrTorch)\n",
    "- [x] Mobile optimizer of Pytorch  --> Runs on CPU but not on GPU\n",
    "- [x] ONNX and it's optimizer\n",
    "- [x] ONNX runtime\n",
    "- [x] TensorRT\n",
    "- [ ] TVM\n",
    "- [x] Convert ONNX model to Tenseoflow and compare runtime. must have align_corners=True in upsample for this\n",
    "\n",
    "Run this notebook interactively  [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/zshn25/Pydnet-Pytorch/c2262edb5b5f86bca5bc2c3bd25b11bd7f69c627?filepath=inference.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libs, load model and example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch # v 1.6\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.utils.mobile_optimizer as mobile_optimizer\n",
    "import numpy as np\n",
    "import tensorflow as tf # 2.4.0-dev\n",
    "\n",
    "from pydnet import PyddepthInference, Pydnet\n",
    "from utils import *\n",
    "\n",
    "device = \"cuda:1\"\n",
    "\n",
    "print(\"Pytorch version:\", torch.__version__, \"\\nTensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load an example input data to run experiments on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='test/zero.png'\n",
    "img1 = Image.open(filepath).convert('RGB')\n",
    "imsize = (448,256) if \"zero\" in filepath else (640,192)\n",
    "img1 = img1.resize(imsize, Image.LANCZOS)\n",
    "#img1=img1.rotate(15)\n",
    "example1 = TF.to_tensor(img1).unsqueeze_(0).to(device)\n",
    "\n",
    "example1_tf = example1.to('cpu').detach().numpy().astype(np.float32)\n",
    "#example1_tf = np.transpose(example1_tf, [0, 2, 3, 1])\n",
    "example1_tf.shape\n",
    "\n",
    "print(example1.shape)\n",
    "plt.imshow(img1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyddepth = PyddepthInference(enc_version=\"resnet18\", dec_version=\"general\", pretrained=True,max_depth=80)\n",
    "#state_dict = torch.load(\"../../../../logs/models/resnet18_general_roll.pth\")\n",
    "#pyddepth.load_state_dict(state_dict)\n",
    "pyddepth.to(device)\n",
    "pyddepth.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on baseline original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pyddepth(example1)\n",
    "\n",
    "%timeit time_torch_model(pyddepth, example1, print_time=False)\n",
    "\n",
    "plot_input_output(img1, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Optimizations for faster inference\n",
    "\n",
    "### 2.1 Model pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Collect parameters to prune\n",
    "parameters_to_prune = ()\n",
    "for name, module in pyddepth.named_modules():\n",
    "      if hasattr(module, \"weight\"):\n",
    "        parameters_to_prune += ((module, 'weight'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.35\n",
    ")\n",
    "\n",
    "for p,_ in parameters_to_prune:\n",
    "    prune.remove(p, name=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pyddepth(example1)\n",
    "\n",
    "%timeit time_torch_model(pyddepth, example1, print_time=False)\n",
    "\n",
    "plot_input_output(img1, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the global sparsity\n",
    "num=0;den=0\n",
    "for name, module in pyddepth.named_modules():\n",
    "      if hasattr(module, \"weight\"):\n",
    "            num += torch.sum(module.weight == 0)\n",
    "            den += module.weight.nelement()\n",
    "            \n",
    "print(\"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(num) / float(den)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pyddepth, \"mobile_pydnet_pruned35.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 FP16 quantization (Half precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyddepth_half = pyddepth.half()\n",
    "example1_half = example1.half()\n",
    "\n",
    "output=pyddepth_half(example1_half)\n",
    "\n",
    "%timeit time_torch_model(pyddepth_half, example1_half, print_time=False)\n",
    "\n",
    "plot_input_output(img1, output.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Jit Trace\n",
    "\n",
    "Jit tracing the FP16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(pyddepth_half.half(), example1_half)\n",
    "\n",
    "output=traced_model(example1_half)\n",
    "%timeit time_torch_model(traced_model,example1_half, print_time=False)\n",
    "\n",
    "plot_input_output(img1, output.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jit tracing the FP32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyddepth.to(device, dtype=torch.float32) # Convert back the original model to fp32\n",
    "traced_model = torch.jit.trace(pyddepth, example1)\n",
    "\n",
    "output=traced_model(example1)\n",
    "%timeit time_torch_model(traced_model,example1, print_time=False )\n",
    "\n",
    "plot_input_output(img1, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripted_model = torch.jit.script(pyddepth, example1)\n",
    "#%timeit scripted_model(example1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIT Traced model is faster than original model. Using JIT traced from now on.\n",
    "\n",
    "### 2.4 Pytorch's mobile optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_traced_model = mobile_optimizer.optimize_for_mobile(traced_model)\n",
    "\n",
    "#%timeit time_torch_model(optimized_traced_model, example1, print_time=False, use_cuda=\"cuda\" in device ) # check https://discuss.pytorch.org/t/runtimeerror-mobile-optimized-model-cannot-be-inferenced-on-gpu/94098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch's mobile optimizer optimizes the model to run on ARM CPU, but not on a GPU. So, when we try to run it on GPU, we get an error but if we try to run on GPU, it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=mobile_optimizer.optimize_for_mobile(traced_model.to(\"cpu\"))(example1.to(\"cpu\"))\n",
    "plot_input_output(img1,output)\n",
    "traced_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3 Model conversions\n",
    "\n",
    "### 3.1 ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model. \n",
    "# Using opset version 11 as the model contains nn.Upsample (which is supported by opset version >=11)\n",
    "onnx_model = torch.onnx.export(traced_model,               # model being run\n",
    "                              example1,                         # model input (or a tuple for multiple inputs)\n",
    "                              \"resnet18_general_roll.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                              example_outputs=output,\n",
    "                              export_params=True,        # store the trained parameter weights inside the model file\n",
    "                              opset_version=11,          # the ONNX version to export the model to\n",
    "                              keep_initializers_as_inputs=True,\n",
    "                              do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                              input_names = ['input'],   # the model's input names\n",
    "                              output_names = ['output'], # the model's output names\n",
    "                              dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                                            'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNXRuntime-GPU requires CUDA 10.1\n",
    "#!pip install onnxruntime\n",
    "#!conda install -c conda-forge onnx --yes\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"ONNX version:\", onnx.__version__, \"\\nONNX Runtime version:\", ort.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"resnet18_general_roll.onnx\")\n",
    "# model = onnx.load(\"/home/e2r/Downloads/pyd_yolov5_448x256.onnx\")\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "#onnx.helper.printable_graph(model.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "#netron.start(\"mobile_pydnet.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 ONNX Optimizer: See [this](https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#optimizing-an-onnx-model)\n",
    "\n",
    "Before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_input = example1.to('cpu').detach().numpy().astype(np.float32)\n",
    "so = ort.SessionOptions()\n",
    "so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "ort_session = ort.InferenceSession('resnet18_general_roll.onnx', so)\n",
    "\n",
    "print(\"This is on CPU as ONNXRuntime is on CPU\")\n",
    "outputs = ort_session.run(None,  {ort_session.get_inputs()[0].name: ort_input})[0]\n",
    "\n",
    "%timeit ort_session.run(None,  {ort_session.get_inputs()[0].name: ort_input})[0]\n",
    "\n",
    "plot_input_output(np.array(img1), outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import optimizer\n",
    "\n",
    "onnx.checker.check_model(model)\n",
    "onnx.helper.strip_doc_string(model)\n",
    "optimized_model = onnx.shape_inference.infer_shapes(model)\n",
    "\n",
    "optimizers_list = ['eliminate_deadend', 'eliminate_nop_dropout',\n",
    "                                            'eliminate_nop_monotone_argmax', 'eliminate_nop_pad',\n",
    "                                            'extract_constant_to_initializer', 'eliminate_unused_initializer',\n",
    "                                            'eliminate_nop_transpose', \n",
    "                                            # disable this optimizer until https://github.com/onnx/optimizer/issues/3 gets fixed\n",
    "                                            'fuse_add_bias_into_conv',\n",
    "                                            'fuse_consecutive_concats',\n",
    "                                            'fuse_consecutive_log_softmax',\n",
    "                                            'fuse_consecutive_reduce_unsqueeze', 'fuse_consecutive_squeezes',\n",
    "                                            'fuse_consecutive_transposes', 'fuse_matmul_add_bias_into_gemm',\n",
    "                                            'fuse_pad_into_conv', 'fuse_transpose_into_gemm']\n",
    "optimized_model = optimizer.optimize(optimized_model, optimizers_list,#optimizer.get_available_passes(),\n",
    "                                     fixed_point=True)\n",
    "onnx.checker.check_model(optimized_model)\n",
    "\n",
    "onnx.save(optimized_model, \"simplified_resnet18_general_roll.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ONNX Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_input = example1.to('cpu').detach().numpy().astype(np.float32)\n",
    "\n",
    "so = ort.SessionOptions()\n",
    "so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "ort_session = ort.InferenceSession('simplified_resnet18_general_roll.onnx', so)\n",
    "#ort_session.set_providers(['CUDAExecutionProvider'])\n",
    "\n",
    "print(\"This is on CPU as ONNXRuntime is on CPU\")\n",
    "outputs = ort_session.run(None,  {ort_session.get_inputs()[0].name: ort_input})[0]\n",
    "\n",
    "%timeit ort_session.run(None,  {ort_session.get_inputs()[0].name: ort_input})[0]\n",
    "\n",
    "plot_input_output(np.array(img1), outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pytorch to Keras\n",
    "#!pip install onnx2keras\n",
    "# from onnx2keras import onnx_to_keras\n",
    "# k_model = onnx_to_keras(onnx_model=model, input_names=['input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convert to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"simplified_resnet18_general_roll.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow --force-reinstall\n",
    "#!pip install tensorflow-addons\n",
    "# Install onnx-tensorflow as follows in terminal\n",
    "#!pip uninstall onnx-tf --yes\n",
    "#!pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
    "#!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow\n",
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "#model = onnx.load(\"mobile_pydnet_interp.onnx\")\n",
    "\n",
    "tf_rep = prepare(model)\n",
    "\n",
    "# Input nodes to the model\n",
    "print('inputs:', tf_rep.inputs)\n",
    "\n",
    "# Output nodes from the model\n",
    "print('outputs:', tf_rep.outputs)\n",
    "\n",
    "# All nodes in the model\n",
    "print('tensor_dict:')\n",
    "print(tf_rep.tensor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow supports inputs in the format `NHWC` but our inputs are in the format `NCHW`. So, we change our input shape as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is on CPU as ONNXRuntime is on CPU\")\n",
    "%timeit tf_rep.run(example1_tf)\n",
    "\n",
    "output1 = tf_rep.run(example1_tf)[0].squeeze()\n",
    "plot_input_output(img1, output1)\n",
    "\n",
    "# # export tensorFlow backend to tensorflow tf file\n",
    "tf_rep.export_graph('simplified_resnet18_general_roll/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was using ONNX for inference. Next we will use Tensorflow for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using Tensorflow # must be v2\n",
    "\n",
    "# Run inference on SavedModel using Tensorflow\n",
    "imported = tf.saved_model.load(\"simplified_resnet18_general_roll/\") # tf.keras.models.load_model('mobile_pydnet_pruned35')\n",
    "output1 = imported(input=example1_tf)[0].numpy().squeeze()\n",
    "\n",
    "%timeit imported(input=example1_tf)\n",
    "\n",
    "plot_input_output(img1, output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorflo model optimization\n",
    "# from tensorflow.python.tools import optimize_for_inference_lib\n",
    "# input_graph_def = graph_pb2.GraphDef()\n",
    "# output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "\n",
    "#       input_graph_def,\n",
    "#       FLAGS.input_names.split(\",\"),\n",
    "#       FLAGS.output_names.split(\",\"), FLAGS.placeholder_type_enum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Convert to Tensorflow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.executing_eagerly()\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"simplified_resnet18_general_roll\")\n",
    "converter.experimental_new_converter = True\n",
    "converter.experimental_new_quantizer = True\n",
    "converter.allow_custom_ops = True\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS,\n",
    "                                      ]#tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.target_spec.supported_types = [tf.int8]\n",
    "# converter.inference_input_type = tf.int8  # or tf.int8\n",
    "# converter.inference_output_type = tf.int8  # or tf.int8\n",
    "# converter.representative_dataset = lambda : [example1_tf]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with tf.io.gfile.GFile('simplified_resnet18_general_roll.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"simplified_resnet18_general_roll.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32) # example1.to('cpu').detach().numpy().astype(np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], example1_tf)\n",
    "\n",
    "print(input_shape)\n",
    "%timeit interpreter.invoke()\n",
    "print(\"done\")\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "\n",
    "output1 = interpreter.get_tensor(output_details[0]['index']).squeeze()\n",
    "\n",
    "plot_input_output(img1, output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Although the above model is converted to tflite, it is not supported by Google Coral TPU, because to support on that hardware, it was necessary to quantize the model to int8 but I couldn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Convert to TensorRT\n",
    "\n",
    "Install [TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html) and [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_to_half(model):\n",
    "    \"\"\"\n",
    "    Convert model to half precision in a batchnorm-safe way.\n",
    "    \"\"\"\n",
    "    def bn_to_float(module):\n",
    "        \"\"\"\n",
    "        BatchNorm layers need parameters in single precision. Find all layers and convert\n",
    "        them back to float.\n",
    "        \"\"\"\n",
    "        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
    "            module.float()\n",
    "        for child in module.children():\n",
    "            bn_to_float(child)\n",
    "        return module\n",
    "    return bn_to_float(model.half())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch2trt import torch2trt\n",
    "\n",
    "model_trt = torch2trt(pyddepth, [example1])\n",
    "output=model_trt(example1)\n",
    "\n",
    "%timeit time_torch_model(model_trt,example1, print_time=False, use_cuda=\"cuda\" in device )\n",
    "\n",
    "plot_input_output(img1, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half precision TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trt = torch2trt(pyddepth.half(), [example1.half()])\n",
    "output=model_trt(example1.half(), fp16_mode=True)\n",
    "\n",
    "#%timeit time_torch_model(model_trt, example1, print_time=False, use_cuda=\"cuda\" in device )\n",
    "\n",
    "plot_input_output(img1, output.double())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Legacy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert from pb to SavedModel\n",
    "\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "export_dir = 'saved_nopad'\n",
    "graph_pb = 'mobile_pydnet_pruned35/saved_model.pb'\n",
    "\n",
    "# builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "\n",
    "with tf.gfile.GFile(graph_pb, \"rb\") as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "sigs = {}\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.import_graph_def(graph_def, name=\"\")\n",
    "    g = tf.get_default_graph()\n",
    "    inp = g.get_tensor_by_name('input:0')\n",
    "    out = g.get_tensor_by_name(\"output:0\")\n",
    "\n",
    "#     sigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \\\n",
    "#         tf.compat.v1.saved_model.signature_def_utils.predict_signature_def(\n",
    "#             {\"input\": inp}, {\"output\": out})\n",
    "\n",
    "#     builder.add_meta_graph_and_variables(sess,\n",
    "#                                          [tag_constants.SERVING],\n",
    "#                                          signature_def_map=sigs)\n",
    "#     builder.save()\n",
    "    #input_tensor_shape = sess.graph.get_tensor_by_name('input:0').shape.as_list()\n",
    "    output = sess.run(out, {\"input:0\": example1.to('cpu').detach().numpy()}) # --> Inference\n",
    "    \n",
    "    g.finalize()\n",
    "\n",
    "\n",
    "#converter = tf.lite.TFLiteConverter.from_saved_model(\"saved\")\n",
    "#tflite_model = converter.convert()\n",
    "    \n",
    "#     [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "\n",
    "# for i in tf.get_default_graph().get_operations():\n",
    "#     print(i)\n",
    "# print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(output.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.test(data = x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#tf.enable_control_flow_v2()\n",
    "converter = tf.lite.TFLiteConverter.from_frozen_graph('mobile_pydnet_pruned35.pb', #TensorFlow freezegraph .pb model file\n",
    "                                                      input_arrays=['input'], # name of input arrays as defined in torch.onnx.export function before.\n",
    "                                                      output_arrays=['output']  # name of output arrays defined in torch.onnx.export function before.\n",
    "                                                      )\n",
    "#converter.experimental_new_converter = True\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# tell converter which type of optimization techniques to use\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# to view the best option for optimization read documentation of tflite about optimization\n",
    "# go to this link https://www.tensorflow.org/lite/guide/get_started#4_optimize_your_model_optional\n",
    "\n",
    "# convert the model \n",
    "tf_lite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "func = tf.saved_model.load('.')\n",
    "#.signatures[\"serving_default\"] \n",
    "out = func( tf.constant(10,tf.float32) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with tf.compat.v1.gfile.GFile('mobile_pydnet_pruned35.pb', \"rb\") as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "    tf.io.write_graph(graph_def, 'tmp/', 'hashtable.pbtxt')\n",
    "    \n",
    "#data_input = tf.placeholder(name='input', dtype=tf.float32, shape=[None, 192, 640, 3])\n",
    "inpu = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "emb = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "phase = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "tf.saved_model.simple_save(sess,\"..\\\\teste_model_2\\\\\",inputs={\"input\":inpu,\"phase\":phase},outputs={\"output\":emb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(\".\")\n",
    "concrete_func = model.signatures[\n",
    "  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "concrete_func.inputs[0].set_shape([1, 192, 640, 3])\n",
    "converter = TFLiteConverter.from_concrete_functions([concrete_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#graph_def = tf.get_default_graph().as_graph_def()\n",
    "\n",
    "with tf.control_dependencies([tf.compat.v1.initializers.tables_initializer()]):\n",
    "      input_int64_tensor = tf.compat.v1.placeholder(tf.int64, shape=[1])\n",
    "      input_string_tensor = tf.compat.v1.placeholder(tf.string, shape=[1])\n",
    "      out_string_tensor = int64_to_string_table.lookup(input_int64_tensor)\n",
    "      out_int64_tensor = string_to_int64_table.lookup(input_string_tensor)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter(graph,\n",
    "                                  [input_int64_tensor, input_string_tensor],\n",
    "                                  [out_string_tensor, out_int64_tensor])\n",
    "\n",
    "supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter.target_spec.supported_ops = supported_ops\n",
    "converter.allow_custom_ops = True\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(graph)\n",
    "frozen_func.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('mobile_pydnet_pruned35.pb', #TensorFlow freezegraph .pb model file\n",
    "                                                      input_arrays=['input'], # name of input arrays as defined in torch.onnx.export function before.\n",
    "                                                      output_arrays=['output']  # name of output arrays defined in torch.onnx.export function before.\n",
    "                                                      )\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(os.path.join(cwd,\"mobile_pydnet_pruned35.pb\"))\n",
    "concrete_func = model.signatures[\n",
    "  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "concrete_func.inputs[0].set_shape([1, 192, 640, 3])\n",
    "converter = TFLiteConverter.from_concrete_functions([concrete_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert the model to Tensorflow Lite.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('mobile_pydnet_pruned35.pb')\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('e2r': conda)",
   "language": "python",
   "name": "python37764bite2rcondad9123364716e49bdbcac99e0b1d6c310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
