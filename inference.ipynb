{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of Pydnet-Pytorch pretrained model on an image\n",
    "\n",
    "Run inference on an image and display the output and test the runtime on local machine: Pytorch 1.6, CUDA 10.1 SuperServer with GTX 2080 TI GPU, Tensorflow 2.4.0-dev.\n",
    "\n",
    "Also optimize the model for faster inference using \n",
    "\n",
    "- [x] JIT from Pytorch\n",
    "- [ ] Jit from NVIDIA (TrTorch)\n",
    "- [ ] Mobile optimizer of Pytorch  --> Gives error. Try blacklisting each of the optimization and check if error disappears\n",
    "- [x] ONNX and it's optimizer\n",
    "- [x] ONNX runtime\n",
    "- [ ] TensorRT\n",
    "- [ ] TVM\n",
    "- [x] Convert ONNX model to Tenseoflow and compare runtime. must have align_corners=True in upsample for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch # v 1.6\n",
    "import torchvision.transforms.functional as TF\n",
    "from pydnet import PyddepthInference, Pydnet\n",
    "import torch.utils.mobile_optimizer as mobile_optimizer\n",
    "import numpy as np\n",
    "import tensorflow as tf # 2.4.0-dev\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNXRuntime-GPU requires CUDA 10.1\n",
    "#!pip install onnxruntime\n",
    "#!conda install -c conda-forge onnx --yes\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.open('test/1.png')\n",
    "img1 = img1.resize((640, 192), Image.ANTIALIAS)\n",
    "example1 = TF.to_tensor(img1).unsqueeze_(0).to(device)\n",
    "\n",
    "example1_tf = example1.to('cpu').detach().numpy().astype(np.float32)\n",
    "#example1_tf = np.transpose(example1_tf, [0, 2, 3, 1])\n",
    "example1_tf.shape\n",
    "\n",
    "print(example1.shape)\n",
    "plt.imshow(img1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bla=Pydnet(mobile_version=True, my_version=False)\n",
    "# loaded_dict = torch.utils.model_zoo.load_url(\"https://github.com/zshn25/Pydnet-Pytorch/blob/forMonodepth2/mobile_pydnet.pth\", \n",
    "#                                              map_location= lambda storage, loc: storage)\n",
    "\n",
    "#bla.load_state_dict(loaded_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyddepth = PyddepthInference(mobile_version=True, my_version=False, pretrained=False)\n",
    "\n",
    "# loaded_dict_enc = torch.load(\"mobile_pydnet.pth\", map_location=device)\n",
    "\n",
    "pyddepth = PyddepthInference(pretrained=True)\n",
    "\n",
    "# loaded_dict_enc = torch.load(\"mobile_pydnet.pth\", map_location=device)\n",
    "\n",
    "# new_dict_enc = {}\n",
    "# for k,v in loaded_dict_enc.items():\n",
    "#     new_dict_enc[k.replace(\"module.\", \"\")] = loaded_dict_enc[k]\n",
    "\n",
    "# pyddepth.load_state_dict(new_dict_enc, strict=False)\n",
    "pyddepth.to(device)\n",
    "pyddepth.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit with torch.no_grad(): pyddepth(example1)\n",
    "\n",
    "output=pyddepth(example1)\n",
    "output1 = output.to('cpu').detach().numpy()\n",
    "output1 = output1.squeeze()\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Collect parameters to prune\n",
    "parameters_to_prune = ()\n",
    "for name, module in pyddepth.named_modules():\n",
    "      if hasattr(module, \"weight\"):\n",
    "        parameters_to_prune += ((module, 'weight'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.35\n",
    ")\n",
    "\n",
    "for p,_ in parameters_to_prune:\n",
    "    prune.remove(p, name=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit with torch.no_grad(): pyddepth(example1)\n",
    "\n",
    "output=pyddepth(example1)\n",
    "output1 = output.to('cpu').detach().numpy()\n",
    "output1 = output1.squeeze()\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the global sparsity\n",
    "num=0;den=0\n",
    "for name, module in pyddepth.named_modules():\n",
    "      if hasattr(module, \"weight\"):\n",
    "            num += torch.sum(module.weight == 0)\n",
    "            den += module.weight.nelement()\n",
    "            \n",
    "print(\"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(num) / float(den)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pyddepth, \"mobile_pydnet_pruned35.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Optimization for faster inference\n",
    "\n",
    "### Jit Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(pyddepth, example1)\n",
    "%timeit traced_model(example1)\n",
    "\n",
    "output=traced_model(example1)\n",
    "output1 = output.to('cpu').detach().numpy()\n",
    "output1 = output1.squeeze()\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripted_model = torch.jit.script(pyddepth, example1)\n",
    "#%timeit scripted_model(example1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIT Traced model is faster than original model. Using JIT traced from now on.\n",
    "\n",
    "### Pytorch's mobile optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_traced_model = mobile_optimizer.optimize_for_mobile(traced_model)\n",
    "\n",
    "#%timeit with torch.no_grad(): optimized_traced_model(example1) # check https://discuss.pytorch.org/t/runtimeerror-mobile-optimized-model-cannot-be-inferenced-on-gpu/94098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Export model. \n",
    "# Using opset version 11 as the model contains nn.Upsample (which is supported by opset version >=11)\n",
    "onnx_model = torch.onnx.export(traced_model,               # model being run\n",
    "                              example1,                         # model input (or a tuple for multiple inputs)\n",
    "                              \"mobile_pydnet.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                              example_outputs=output,\n",
    "                              export_params=True,        # store the trained parameter weights inside the model file\n",
    "                              opset_version=11,          # the ONNX version to export the model to\n",
    "                              keep_initializers_as_inputs=True,\n",
    "                              do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                              input_names = ['input'],   # the model's input names\n",
    "                              output_names = ['output'], # the model's output names\n",
    "                              dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                                            'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNXRuntime-GPU requires CUDA 10.1\n",
    "#!pip install onnxruntime\n",
    "#!conda install -c conda-forge onnx --yes\n",
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"mobile_pydnet.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "#onnx.helper.printable_graph(model.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "#netron.start(\"mobile_pydnet.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX Optimizer: See [this](https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md#optimizing-an-onnx-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import optimizer\n",
    "\n",
    "onnx.checker.check_model(model)\n",
    "onnx.helper.strip_doc_string(model)\n",
    "optimized_model = onnx.shape_inference.infer_shapes(model)\n",
    "\n",
    "optimizers_list = ['eliminate_deadend', 'eliminate_nop_dropout',\n",
    "                                            'eliminate_nop_monotone_argmax', 'eliminate_nop_pad',\n",
    "                                            'extract_constant_to_initializer', 'eliminate_unused_initializer',\n",
    "                                            'eliminate_nop_transpose', \n",
    "                                            # disable this optimizer until https://github.com/onnx/optimizer/issues/3 gets fixed\n",
    "                                            'fuse_add_bias_into_conv',\n",
    "                                            'fuse_consecutive_concats',\n",
    "                                            'fuse_consecutive_log_softmax',\n",
    "                                            'fuse_consecutive_reduce_unsqueeze', 'fuse_consecutive_squeezes',\n",
    "                                            'fuse_consecutive_transposes', 'fuse_matmul_add_bias_into_gemm',\n",
    "                                            'fuse_pad_into_conv', 'fuse_transpose_into_gemm']\n",
    "optimized_model = optimizer.optimize(optimized_model, optimizers_list,#optimizer.get_available_passes(),\n",
    "                                     fixed_point=True)\n",
    "onnx.checker.check_model(optimized_model)\n",
    "\n",
    "onnx.save(optimized_model, \"optimized_mobile_pydnet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ort_input = example1.to('cpu').detach().numpy().astype(np.float32)\n",
    "\n",
    "ort_session = ort.InferenceSession('optimized_mobile_pydnet.onnx')\n",
    "\n",
    "#print(\"This is on CPU as ONNXRuntime is on CPU\")\n",
    "%timeit outputs = ort_session.run(None,  {ort_session.get_inputs()[0].name: ort_input})[0]\n",
    "\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pytorch to Keras\n",
    "#!pip install onnx2keras\n",
    "# from onnx2keras import onnx_to_keras\n",
    "# k_model = onnx_to_keras(onnx_model=model, input_names=['input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"optimized_mobile_pydnet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow --force-reinstall\n",
    "#!pip install tensorflow-addons\n",
    "# Install onnx-tensorflow as follows in terminal\n",
    "#!pip uninstall onnx-tf --yes\n",
    "#!pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
    "#!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow\n",
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "#model = onnx.load(\"mobile_pydnet_interp.onnx\")\n",
    "\n",
    "tf_rep = prepare(model)\n",
    "\n",
    "# Input nodes to the model\n",
    "print('inputs:', tf_rep.inputs)\n",
    "\n",
    "# Output nodes from the model\n",
    "print('outputs:', tf_rep.outputs)\n",
    "\n",
    "# All nodes in the model\n",
    "print('tensor_dict:')\n",
    "print(tf_rep.tensor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow supports inputs in the format `NHWC` but our inputs are in the format `NCHW`. So, we change our input shape as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is on CPU as ONNXRuntime is on CPU\")\n",
    "%timeit tf_rep.run(example1_tf)\n",
    "\n",
    "output1 = tf_rep.run(example1_tf)[0].squeeze()\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);\n",
    "\n",
    "# # export tensorFlow backend to tensorflow tf file\n",
    "tf_rep.export_graph('mobile_pydnet_pruned35/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using Tensorflow # must be v2\n",
    "\n",
    "# Run inference on SavedModel using Tensorflow\n",
    "imported = tf.saved_model.load(\"mobile_pydnet_pruned35/\") # tf.keras.models.load_model('mobile_pydnet_pruned35')\n",
    "%timeit imported(input=example1_tf)\n",
    "\n",
    "output1 = imported(input=example1.to('cpu').detach().numpy().astype(np.float32))[0].numpy().squeeze()\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorflo model optimization\n",
    "# from tensorflow.python.tools import optimize_for_inference_lib\n",
    "# input_graph_def = graph_pb2.GraphDef()\n",
    "# output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "\n",
    "#       input_graph_def,\n",
    "#       FLAGS.input_names.split(\",\"),\n",
    "#       FLAGS.output_names.split(\",\"), FLAGS.placeholder_type_enum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensorflow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"mobile_pydnet_pruned35\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with tf.io.gfile.GFile('mobile_pydnet_pruned35.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"mobile_pydnet_pruned35.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32) # example1.to('cpu').detach().numpy().astype(np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], examplee1_tf)\n",
    "\n",
    "print(input_shape)\n",
    "%timeit interpreter.invoke()\n",
    "print(\"done\")\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "\n",
    "output1 = interpreter.get_tensor(output_details[0]['index']).squeeze()\n",
    "print(output1.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,3))\n",
    "axes[0].imshow(img1)\n",
    "depthmap=axes[1].imshow(output1)\n",
    "fig.colorbar(depthmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Legacy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from pb to SavedModel\n",
    "\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "export_dir = 'saved_nopad'\n",
    "graph_pb = 'mobile_pydnet_pruned35/saved_model.pb'\n",
    "\n",
    "# builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "\n",
    "with tf.gfile.GFile(graph_pb, \"rb\") as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "sigs = {}\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.import_graph_def(graph_def, name=\"\")\n",
    "    g = tf.get_default_graph()\n",
    "    inp = g.get_tensor_by_name('input:0')\n",
    "    out = g.get_tensor_by_name(\"output:0\")\n",
    "\n",
    "#     sigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \\\n",
    "#         tf.compat.v1.saved_model.signature_def_utils.predict_signature_def(\n",
    "#             {\"input\": inp}, {\"output\": out})\n",
    "\n",
    "#     builder.add_meta_graph_and_variables(sess,\n",
    "#                                          [tag_constants.SERVING],\n",
    "#                                          signature_def_map=sigs)\n",
    "#     builder.save()\n",
    "    #input_tensor_shape = sess.graph.get_tensor_by_name('input:0').shape.as_list()\n",
    "    output = sess.run(out, {\"input:0\": example1.to('cpu').detach().numpy()}) # --> Inference\n",
    "    \n",
    "    g.finalize()\n",
    "\n",
    "\n",
    "#converter = tf.lite.TFLiteConverter.from_saved_model(\"saved\")\n",
    "#tflite_model = converter.convert()\n",
    "    \n",
    "#     [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "\n",
    "# for i in tf.get_default_graph().get_operations():\n",
    "#     print(i)\n",
    "# print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(data = x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_control_flow_v2()\n",
    "converter = tf.lite.TFLiteConverter.from_frozen_graph('mobile_pydnet_pruned35.pb', #TensorFlow freezegraph .pb model file\n",
    "                                                      input_arrays=['input'], # name of input arrays as defined in torch.onnx.export function before.\n",
    "                                                      output_arrays=['output']  # name of output arrays defined in torch.onnx.export function before.\n",
    "                                                      )\n",
    "#converter.experimental_new_converter = True\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "# tell converter which type of optimization techniques to use\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# to view the best option for optimization read documentation of tflite about optimization\n",
    "# go to this link https://www.tensorflow.org/lite/guide/get_started#4_optimize_your_model_optional\n",
    "\n",
    "# convert the model \n",
    "tf_lite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = tf.saved_model.load('.')\n",
    "#.signatures[\"serving_default\"] \n",
    "out = func( tf.constant(10,tf.float32) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.compat.v1.gfile.GFile('mobile_pydnet_pruned35.pb', \"rb\") as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "with tf.Graph().as_default() as graph:\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "    tf.io.write_graph(graph_def, 'tmp/', 'hashtable.pbtxt')\n",
    "    \n",
    "#data_input = tf.placeholder(name='input', dtype=tf.float32, shape=[None, 192, 640, 3])\n",
    "inpu = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "emb = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "phase = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "tf.saved_model.simple_save(sess,\"..\\\\teste_model_2\\\\\",inputs={\"input\":inpu,\"phase\":phase},outputs={\"output\":emb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(\".\")\n",
    "concrete_func = model.signatures[\n",
    "  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "concrete_func.inputs[0].set_shape([1, 192, 640, 3])\n",
    "converter = TFLiteConverter.from_concrete_functions([concrete_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_def = tf.get_default_graph().as_graph_def()\n",
    "\n",
    "with tf.control_dependencies([tf.compat.v1.initializers.tables_initializer()]):\n",
    "      input_int64_tensor = tf.compat.v1.placeholder(tf.int64, shape=[1])\n",
    "      input_string_tensor = tf.compat.v1.placeholder(tf.string, shape=[1])\n",
    "      out_string_tensor = int64_to_string_table.lookup(input_int64_tensor)\n",
    "      out_int64_tensor = string_to_int64_table.lookup(input_string_tensor)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter(graph,\n",
    "                                  [input_int64_tensor, input_string_tensor],\n",
    "                                  [out_string_tensor, out_int64_tensor])\n",
    "\n",
    "supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter.target_spec.supported_ops = supported_ops\n",
    "converter.allow_custom_ops = True\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(graph)\n",
    "frozen_func.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('mobile_pydnet_pruned35.pb', #TensorFlow freezegraph .pb model file\n",
    "                                                      input_arrays=['input'], # name of input arrays as defined in torch.onnx.export function before.\n",
    "                                                      output_arrays=['output']  # name of output arrays defined in torch.onnx.export function before.\n",
    "                                                      )\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(os.path.join(cwd,\"mobile_pydnet_pruned35.pb\"))\n",
    "concrete_func = model.signatures[\n",
    "  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "concrete_func.inputs[0].set_shape([1, 192, 640, 3])\n",
    "converter = TFLiteConverter.from_concrete_functions([concrete_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to Tensorflow Lite.\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('mobile_pydnet_pruned35.pb')\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
